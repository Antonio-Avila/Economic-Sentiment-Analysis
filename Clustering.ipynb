{"cells":[{"cell_type":"markdown","source":"# Clustering","metadata":{"tags":[],"cell_id":"5a57f44f-bb41-40e7-b98e-91f0056ac0a3"}},{"cell_type":"markdown","source":"## K-Means Clustering","metadata":{"tags":[],"cell_id":"4d6cef86-2bea-49e6-8e3c-7cf630f320e8"}},{"cell_type":"code","metadata":{"tags":[],"cell_id":"bcefc239-388f-4ff7-9239-d486a027e160"},"source":"from gensim.models import Doc2Vec\r\nfrom sklearn.cluster import KMeans\r\nimport numpy as np\r\nimport pandas as pd\r\nimport time","execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"module 'numpy.linalg.lapack_lite' has no attribute '_ilp64'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-75f18e1a98fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/gensim/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparsing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msimilarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummarization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/gensim/corpora/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# bring corpus classes directly into package namespace, to save some typing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mindexedcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedCorpus\u001b[0m  \u001b[0;31m# noqa:F401 must appear before the other classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmmcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMmCorpus\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/gensim/corpora/indexedcorpus.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterfaces\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/gensim/interfaces.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/gensim/matutils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mentropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_lapack_funcs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/stats/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \"\"\"\n\u001b[0;32m--> 388\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdistributions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmorestats\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mspecial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmstats_basic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m from ._stats_mstats_common import (_find_repeats, linregress, theilslopes,\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/stats/distributions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#       instead of `git blame -Lxxx,+x`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from ._distn_infrastructure import (entropy, rv_discrete, rv_continuous,\n\u001b[0m\u001b[1;32m      9\u001b[0m                                     rv_frozen)\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/stats/_distn_infrastructure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# for root finding for continuous distribution ppf, and max likelihood estimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# for functions of continuous distributions (e.g. moments, entropy, cdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/optimize/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_minimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_root\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_root_scalar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_trustregion_krylov\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_minimize_trust_krylov\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_trustregion_exact\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_minimize_trustregion_exact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_trustregion_constr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_minimize_trustregion_constr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# constrained minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mminimize_trustregion_constr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_minimize_trustregion_constr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'_minimize_trustregion_constr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVectorFunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m from .._constraints import (\n\u001b[0m\u001b[1;32m      6\u001b[0m     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hessian_update_strategy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBFGS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/scipy/optimize/_constraints.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptimizeWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwarnings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msuppress_warnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/numpy/testing/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munittest\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTestCase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_private\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecorators\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from ._private.nosetester import (\n","\u001b[0;32m/opt/venv/lib/python3.7/site-packages/numpy/testing/_private/utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mIS_PYPY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplatform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_implementation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'PyPy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0mHAS_REFCOUNT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'getrefcount'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mHAS_LAPACK64\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlapack_lite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ilp64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'numpy.linalg.lapack_lite' has no attribute '_ilp64'"]}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"90d49fd7-256a-46d9-9556-47b991bcb0bb"},"source":"dm_model = Doc2Vec.load('./models/nlp models/dm_model')\r\ndbow_model = Doc2Vec.load('./models/nlp models/dbow_model')\r\ndm_mean_model = Doc2Vec.load('./models/nlp models/dm_mean_model')\r\ndm_concat_model = Doc2Vec.load('./models/nlp models/dm_concat_model')","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"263783f5-b9ca-4d3b-81dd-397189443f0a"},"source":"help(dm_model.wv)","execution_count":null,"outputs":[{"name":"stdout","text":"Help on Word2VecKeyedVectors in module gensim.models.keyedvectors object:\n\nclass Word2VecKeyedVectors(WordEmbeddingsKeyedVectors)\n |  Word2VecKeyedVectors(vector_size)\n |  \n |  Mapping between words and vectors for the :class:`~gensim.models.Word2Vec` model.\n |  Used to perform operations on the vectors such as vector lookup, distance, similarity etc.\n |  \n |  Method resolution order:\n |      Word2VecKeyedVectors\n |      WordEmbeddingsKeyedVectors\n |      BaseKeyedVectors\n |      gensim.utils.SaveLoad\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  save_word2vec_format(self, fname, fvocab=None, binary=False, total_vec=None)\n |      Store the input-hidden weight matrix in the same format used by the original\n |      C word2vec-tool, for compatibility.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          The file path used to save the vectors in\n |      fvocab : str, optional\n |          Optional file path used to save the vocabulary\n |      binary : bool, optional\n |          If True, the data will be saved in binary word2vec format, else it will be saved in plain text.\n |      total_vec : int, optional\n |          Optional parameter to explicitly specify total no. of vectors\n |          (in case word vectors are appended with document vectors afterwards).\n |  \n |  ----------------------------------------------------------------------\n |  Class methods defined here:\n |  \n |  load(fname_or_handle, **kwargs) from builtins.type\n |      Load an object previously saved using :meth:`~gensim.utils.SaveLoad.save` from a file.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to file that contains needed object.\n |      mmap : str, optional\n |          Memory-map option.  If the object was saved with large arrays stored separately, you can load these arrays\n |          via mmap (shared memory) using `mmap='r'.\n |          If the file being loaded is compressed (either '.gz' or '.bz2'), then `mmap=None` **must be** set.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.utils.SaveLoad.save`\n |          Save object to file.\n |      \n |      Returns\n |      -------\n |      object\n |          Object loaded from `fname`.\n |      \n |      Raises\n |      ------\n |      AttributeError\n |          When called on an object instance instead of class (this is a class method).\n |  \n |  load_word2vec_format(fname, fvocab=None, binary=False, encoding='utf8', unicode_errors='strict', limit=None, datatype=<class 'numpy.float32'>) from builtins.type\n |      Load the input-hidden weight matrix from the original C word2vec-tool format.\n |      \n |      Warnings\n |      --------\n |      The information stored in the file is incomplete (the binary tree is missing),\n |      so while you can query for word similarity etc., you cannot continue training\n |      with a model loaded this way.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          The file path to the saved word2vec-format file.\n |      fvocab : str, optional\n |          File path to the vocabulary.Word counts are read from `fvocab` filename, if set\n |          (this is the file generated by `-save-vocab` flag of the original C tool).\n |      binary : bool, optional\n |          If True, indicates whether the data is in binary word2vec format.\n |      encoding : str, optional\n |          If you trained the C model using non-utf8 encoding for words, specify that encoding in `encoding`.\n |      unicode_errors : str, optional\n |          default 'strict', is a string suitable to be passed as the `errors`\n |          argument to the unicode() (Python 2.x) or str() (Python 3.x) function. If your source\n |          file may include word tokens truncated in the middle of a multibyte unicode character\n |          (as is common from the original word2vec.c tool), 'ignore' or 'replace' may help.\n |      limit : int, optional\n |          Sets a maximum number of word-vectors to read from the file. The default,\n |          None, means read all.\n |      datatype : type, optional\n |          (Experimental) Can coerce dimensions to a non-default float type (such as `np.float16`) to save memory.\n |          Such types may result in much slower bulk operations or incompatibility with optimized routines.)\n |      \n |      Returns\n |      -------\n |      :class:`~gensim.models.keyedvectors.Word2VecKeyedVectors`\n |          Loaded model.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from WordEmbeddingsKeyedVectors:\n |  \n |  __contains__(self, word)\n |  \n |  __init__(self, vector_size)\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  accuracy(self, questions, restrict_vocab=30000, most_similar=<function WordEmbeddingsKeyedVectors.most_similar at 0x7fca3d2049d8>, case_insensitive=True)\n |      Compute accuracy of the model.\n |      \n |      The accuracy is reported (=printed to log and returned as a list) for each\n |      section separately, plus there's one aggregate summary at the end.\n |      \n |      Parameters\n |      ----------\n |      questions : str\n |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n |          See `gensim/test/test_data/questions-words.txt` as example.\n |      restrict_vocab : int, optional\n |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n |          in modern word embedding models).\n |      most_similar : function, optional\n |          Function used for similarity calculation.\n |      case_insensitive : bool, optional\n |          If True - convert all words to their uppercase form before evaluating the performance.\n |          Useful to handle case-mismatch between training tokens and words in the test set.\n |          In case of multiple case variants of a single word, the vector for the first occurrence\n |          (also the most frequent if vocabulary is sorted) is taken.\n |      \n |      Returns\n |      -------\n |      list of dict of (str, (str, str, str)\n |          Full lists of correct and incorrect predictions divided by sections.\n |  \n |  distance(self, w1, w2)\n |      Compute cosine distance between two words.\n |      Calculate 1 - :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`.\n |      \n |      Parameters\n |      ----------\n |      w1 : str\n |          Input word.\n |      w2 : str\n |          Input word.\n |      \n |      Returns\n |      -------\n |      float\n |          Distance between `w1` and `w2`.\n |  \n |  distances(self, word_or_vector, other_words=())\n |      Compute cosine distances from given word or vector to all words in `other_words`.\n |      If `other_words` is empty, return distance between `word_or_vectors` and all words in vocab.\n |      \n |      Parameters\n |      ----------\n |      word_or_vector : {str, numpy.ndarray}\n |          Word or vector from which distances are to be computed.\n |      other_words : iterable of str\n |          For each word in `other_words` distance from `word_or_vector` is computed.\n |          If None or empty, distance of `word_or_vector` from all words in vocab is computed (including itself).\n |      \n |      Returns\n |      -------\n |      numpy.array\n |          Array containing distances to all words in `other_words` from input `word_or_vector`.\n |      \n |      Raises\n |      -----\n |      KeyError\n |          If either `word_or_vector` or any word in `other_words` is absent from vocab.\n |  \n |  doesnt_match(self, words)\n |      Which word from the given list doesn't go with the others?\n |      \n |      Parameters\n |      ----------\n |      words : list of str\n |          List of words.\n |      \n |      Returns\n |      -------\n |      str\n |          The word further away from the mean of all words.\n |  \n |  evaluate_word_analogies(self, analogies, restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n |      Compute performance of the model on an analogy test set.\n |      \n |      This is modern variant of :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.accuracy`, see\n |      `discussion on GitHub #1935 <https://github.com/RaRe-Technologies/gensim/pull/1935>`_.\n |      \n |      The accuracy is reported (printed to log and returned as a score) for each section separately,\n |      plus there's one aggregate summary at the end.\n |      \n |      This method corresponds to the `compute-accuracy` script of the original C word2vec.\n |      See also `Analogy (State of the art) <https://aclweb.org/aclwiki/Analogy_(State_of_the_art)>`_.\n |      \n |      Parameters\n |      ----------\n |      analogies : str\n |          Path to file, where lines are 4-tuples of words, split into sections by \": SECTION NAME\" lines.\n |          See `gensim/test/test_data/questions-words.txt` as example.\n |      restrict_vocab : int, optional\n |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n |          in modern word embedding models).\n |      case_insensitive : bool, optional\n |          If True - convert all words to their uppercase form before evaluating the performance.\n |          Useful to handle case-mismatch between training tokens and words in the test set.\n |          In case of multiple case variants of a single word, the vector for the first occurrence\n |          (also the most frequent if vocabulary is sorted) is taken.\n |      dummy4unknown : bool, optional\n |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n |      \n |      Returns\n |      -------\n |      score : float\n |          The overall evaluation score on the entire evaluation set\n |      sections : list of dict of {str : str or list of tuple of (str, str, str, str)}\n |          Results broken down by each section of the evaluation set. Each dict contains the name of the section\n |          under the key 'section', and lists of correctly and incorrectly predicted 4-tuples of words under the\n |          keys 'correct' and 'incorrect'.\n |  \n |  evaluate_word_pairs(self, pairs, delimiter='\\t', restrict_vocab=300000, case_insensitive=True, dummy4unknown=False)\n |      Compute correlation of the model with human similarity judgments.\n |      \n |      Notes\n |      -----\n |      More datasets can be found at\n |      * http://technion.ac.il/~ira.leviant/MultilingualVSMdata.html\n |      * https://www.cl.cam.ac.uk/~fh295/simlex.html.\n |      \n |      Parameters\n |      ----------\n |      pairs : str\n |          Path to file, where lines are 3-tuples, each consisting of a word pair and a similarity value.\n |          See `test/test_data/wordsim353.tsv` as example.\n |      delimiter : str, optional\n |          Separator in `pairs` file.\n |      restrict_vocab : int, optional\n |          Ignore all 4-tuples containing a word not in the first `restrict_vocab` words.\n |          This may be meaningful if you've sorted the model vocabulary by descending frequency (which is standard\n |          in modern word embedding models).\n |      case_insensitive : bool, optional\n |          If True - convert all words to their uppercase form before evaluating the performance.\n |          Useful to handle case-mismatch between training tokens and words in the test set.\n |          In case of multiple case variants of a single word, the vector for the first occurrence\n |          (also the most frequent if vocabulary is sorted) is taken.\n |      dummy4unknown : bool, optional\n |          If True - produce zero accuracies for 4-tuples with out-of-vocabulary words.\n |          Otherwise, these tuples are skipped entirely and not used in the evaluation.\n |      \n |      Returns\n |      -------\n |      pearson : tuple of (float, float)\n |          Pearson correlation coefficient with 2-tailed p-value.\n |      spearman : tuple of (float, float)\n |          Spearman rank-order correlation coefficient between the similarities from the dataset and the\n |          similarities produced by the model itself, with 2-tailed p-value.\n |      oov_ratio : float\n |          The ratio of pairs with unknown words.\n |  \n |  get_keras_embedding(self, train_embeddings=False, word_index=None)\n |      Get a Keras 'Embedding' layer with weights set as the Word2Vec model's learned word embeddings.\n |      \n |      Parameters\n |      ----------\n |      train_embeddings : bool\n |          If False, the weights are frozen and stopped from being updated.\n |          If True, the weights can/will be further trained/updated.\n |      \n |      word_index : {str : int}\n |          A mapping from tokens to their indices the way they will be provided in the input to the embedding layer.\n |          The embedding of each token will be placed at the corresponding index in the returned matrix.\n |          Tokens not in the index are ignored.\n |          This is useful when the token indices are produced by a process that is not coupled with the embedding\n |          model, e.x. an Keras Tokenizer object.\n |          If None, the embedding matrix in the embedding layer will be indexed according to self.vocab\n |      \n |      Returns\n |      -------\n |      `keras.layers.Embedding`\n |          Embedding layer.\n |      \n |      Raises\n |      ------\n |      ImportError\n |          If `Keras <https://pypi.org/project/Keras/>`_ not installed.\n |      \n |      Warnings\n |      --------\n |      Current method works only if `Keras <https://pypi.org/project/Keras/>`_ installed.\n |  \n |  get_vector(self, word)\n |      Get the entity's representations in vector space, as a 1D numpy array.\n |      \n |      Parameters\n |      ----------\n |      entity : str\n |          Identifier of the entity to return the vector for.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Vector for the specified entity.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If the given entity identifier doesn't exist.\n |  \n |  init_sims(self, replace=False)\n |      Precompute L2-normalized vectors.\n |      \n |      Parameters\n |      ----------\n |      replace : bool, optional\n |          If True - forget the original vectors and only keep the normalized ones = saves lots of memory!\n |      \n |      Warnings\n |      --------\n |      You **cannot continue training** after doing a replace.\n |      The model becomes effectively read-only: you can call\n |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`,\n |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.similarity`, etc., but not train.\n |  \n |  most_similar(self, positive=None, negative=None, topn=10, restrict_vocab=None, indexer=None)\n |      Find the top-N most similar words.\n |      Positive words contribute positively towards the similarity, negative words negatively.\n |      \n |      This method computes cosine similarity between a simple mean of the projection\n |      weight vectors of the given words and the vectors for each word in the model.\n |      The method corresponds to the `word-analogy` and `distance` scripts in the original\n |      word2vec implementation.\n |      \n |      Parameters\n |      ----------\n |      positive : list of str, optional\n |          List of words that contribute positively.\n |      negative : list of str, optional\n |          List of words that contribute negatively.\n |      topn : int or None, optional\n |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n |          then similarities for all words are returned.\n |      restrict_vocab : int, optional\n |          Optional integer which limits the range of vectors which\n |          are searched for most-similar values. For example, restrict_vocab=10000 would\n |          only check the first 10000 word vectors in the vocabulary order. (This may be\n |          meaningful if you've sorted the vocabulary by descending frequency.)\n |      \n |      Returns\n |      -------\n |      list of (str, float) or numpy.array\n |          When `topn` is int, a sequence of (word, similarity) is returned.\n |          When `topn` is None, then similarities for all words are returned as a\n |          one-dimensional numpy array with the size of the vocabulary.\n |  \n |  most_similar_cosmul(self, positive=None, negative=None, topn=10)\n |      Find the top-N most similar words, using the multiplicative combination objective,\n |      proposed by `Omer Levy and Yoav Goldberg \"Linguistic Regularities in Sparse and Explicit Word Representations\"\n |      <http://www.aclweb.org/anthology/W14-1618>`_. Positive words still contribute positively towards the similarity,\n |      negative words negatively, but with less susceptibility to one large distance dominating the calculation.\n |      In the common analogy-solving case, of two positive and one negative examples,\n |      this method is equivalent to the \"3CosMul\" objective (equation (4)) of Levy and Goldberg.\n |      \n |      Additional positive or negative examples contribute to the numerator or denominator,\n |      respectively - a potentially sensible but untested extension of the method.\n |      With a single positive example, rankings will be the same as in the default\n |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.most_similar`.\n |      \n |      Parameters\n |      ----------\n |      positive : list of str, optional\n |          List of words that contribute positively.\n |      negative : list of str, optional\n |          List of words that contribute negatively.\n |      topn : int or None, optional\n |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n |          then similarities for all words are returned.\n |      \n |      Returns\n |      -------\n |      list of (str, float) or numpy.array\n |          When `topn` is int, a sequence of (word, similarity) is returned.\n |          When `topn` is None, then similarities for all words are returned as a\n |          one-dimensional numpy array with the size of the vocabulary.\n |  \n |  n_similarity(self, ws1, ws2)\n |      Compute cosine similarity between two sets of words.\n |      \n |      Parameters\n |      ----------\n |      ws1 : list of str\n |          Sequence of words.\n |      ws2: list of str\n |          Sequence of words.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Similarities between `ws1` and `ws2`.\n |  \n |  relative_cosine_similarity(self, wa, wb, topn=10)\n |      Compute the relative cosine similarity between two words given top-n similar words,\n |      by `Artuur Leeuwenberga, Mihaela Velab , Jon Dehdaribc, Josef van Genabithbc \"A Minimally Supervised Approach\n |      for Synonym Extraction with Word Embeddings\" <https://ufal.mff.cuni.cz/pbml/105/art-leeuwenberg-et-al.pdf>`_.\n |      \n |      To calculate relative cosine similarity between two words, equation (1) of the paper is used.\n |      For WordNet synonyms, if rcs(topn=10) is greater than 0.10 then wa and wb are more similar than\n |      any arbitrary word pairs.\n |      \n |      Parameters\n |      ----------\n |      wa: str\n |          Word for which we have to look top-n similar word.\n |      wb: str\n |          Word for which we evaluating relative cosine similarity with wa.\n |      topn: int, optional\n |          Number of top-n similar words to look with respect to wa.\n |      \n |      Returns\n |      -------\n |      numpy.float64\n |          Relative cosine similarity between wa and wb.\n |  \n |  save(self, *args, **kwargs)\n |      Save KeyedVectors.\n |      \n |      Parameters\n |      ----------\n |      fname : str\n |          Path to the output file.\n |      \n |      See Also\n |      --------\n |      :meth:`~gensim.models.keyedvectors.WordEmbeddingsKeyedVectors.load`\n |          Load saved model.\n |  \n |  similar_by_vector(self, vector, topn=10, restrict_vocab=None)\n |      Find the top-N most similar words by vector.\n |      \n |      Parameters\n |      ----------\n |      vector : numpy.array\n |          Vector from which similarities are to be computed.\n |      topn : int or None, optional\n |          Number of top-N similar words to return, when `topn` is int. When `topn` is None,\n |          then similarities for all words are returned.\n |      restrict_vocab : int, optional\n |          Optional integer which limits the range of vectors which\n |          are searched for most-similar values. For example, restrict_vocab=10000 would\n |          only check the first 10000 word vectors in the vocabulary order. (This may be\n |          meaningful if you've sorted the vocabulary by descending frequency.)\n |      \n |      Returns\n |      -------\n |      list of (str, float) or numpy.array\n |          When `topn` is int, a sequence of (word, similarity) is returned.\n |          When `topn` is None, then similarities for all words are returned as a\n |          one-dimensional numpy array with the size of the vocabulary.\n |  \n |  similar_by_word(self, word, topn=10, restrict_vocab=None)\n |      Find the top-N most similar words.\n |      \n |      Parameters\n |      ----------\n |      word : str\n |          Word\n |      topn : int or None, optional\n |          Number of top-N similar words to return. If topn is None, similar_by_word returns\n |          the vector of similarity scores.\n |      restrict_vocab : int, optional\n |          Optional integer which limits the range of vectors which\n |          are searched for most-similar values. For example, restrict_vocab=10000 would\n |          only check the first 10000 word vectors in the vocabulary order. (This may be\n |          meaningful if you've sorted the vocabulary by descending frequency.)\n |      \n |      Returns\n |      -------\n |      list of (str, float) or numpy.array\n |          When `topn` is int, a sequence of (word, similarity) is returned.\n |          When `topn` is None, then similarities for all words are returned as a\n |          one-dimensional numpy array with the size of the vocabulary.\n |  \n |  similarity(self, w1, w2)\n |      Compute cosine similarity between two words.\n |      \n |      Parameters\n |      ----------\n |      w1 : str\n |          Input word.\n |      w2 : str\n |          Input word.\n |      \n |      Returns\n |      -------\n |      float\n |          Cosine similarity between `w1` and `w2`.\n |  \n |  similarity_matrix(self, dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100, dtype=<class 'numpy.float32'>)\n |      Construct a term similarity matrix for computing Soft Cosine Measure.\n |      \n |      This creates a sparse term similarity matrix in the :class:`scipy.sparse.csc_matrix` format for computing\n |      Soft Cosine Measure between documents.\n |      \n |      Parameters\n |      ----------\n |      dictionary : :class:`~gensim.corpora.dictionary.Dictionary`\n |          A dictionary that specifies the considered terms.\n |      tfidf : :class:`gensim.models.tfidfmodel.TfidfModel` or None, optional\n |          A model that specifies the relative importance of the terms in the dictionary. The\n |          columns of the term similarity matrix will be build in a decreasing order of importance\n |          of terms, or in the order of term identifiers if None.\n |      threshold : float, optional\n |          Only embeddings more similar than `threshold` are considered when retrieving word\n |          embeddings closest to a given word embedding.\n |      exponent : float, optional\n |          Take the word embedding similarities larger than `threshold` to the power of `exponent`.\n |      nonzero_limit : int, optional\n |          The maximum number of non-zero elements outside the diagonal in a single column of the\n |          sparse term similarity matrix.\n |      dtype : numpy.dtype, optional\n |          Data-type of the sparse term similarity matrix.\n |      \n |      Returns\n |      -------\n |      :class:`scipy.sparse.csc_matrix`\n |          Term similarity matrix.\n |      \n |      See Also\n |      --------\n |      :func:`gensim.matutils.softcossim`\n |          The Soft Cosine Measure.\n |      :class:`~gensim.similarities.docsim.SoftCosineSimilarity`\n |          A class for performing corpus-based similarity queries with Soft Cosine Measure.\n |      \n |      Notes\n |      -----\n |      The constructed matrix corresponds to the matrix Mrel defined in section 2.1 of\n |      `Delphine Charlet and Geraldine Damnati, \"SimBow at SemEval-2017 Task 3: Soft-Cosine Semantic Similarity\n |      between Questions for Community Question Answering\", 2017\n |      <http://www.aclweb.org/anthology/S/S17/S17-2051.pdf>`_.\n |  \n |  wmdistance(self, document1, document2)\n |      Compute the Word Mover's Distance between two documents.\n |      \n |      When using this code, please consider citing the following papers:\n |      \n |      * `Ofir Pele and Michael Werman \"A linear time histogram metric for improved SIFT matching\"\n |        <http://www.cs.huji.ac.il/~werman/Papers/ECCV2008.pdf>`_\n |      * `Ofir Pele and Michael Werman \"Fast and robust earth mover's distances\"\n |        <https://ieeexplore.ieee.org/document/5459199/>`_\n |      * `Matt Kusner et al. \"From Word Embeddings To Document Distances\"\n |        <http://proceedings.mlr.press/v37/kusnerb15.pdf>`_.\n |      \n |      Parameters\n |      ----------\n |      document1 : list of str\n |          Input document.\n |      document2 : list of str\n |          Input document.\n |      \n |      Returns\n |      -------\n |      float\n |          Word Mover's distance between `document1` and `document2`.\n |      \n |      Warnings\n |      --------\n |      This method only works if `pyemd <https://pypi.org/project/pyemd/>`_ is installed.\n |      \n |      If one of the documents have no words that exist in the vocab, `float('inf')` (i.e. infinity)\n |      will be returned.\n |      \n |      Raises\n |      ------\n |      ImportError\n |          If `pyemd <https://pypi.org/project/pyemd/>`_  isn't installed.\n |  \n |  word_vec(self, word, use_norm=False)\n |      Get `word` representations in vector space, as a 1D numpy array.\n |      \n |      Parameters\n |      ----------\n |      word : str\n |          Input word\n |      use_norm : bool, optional\n |          If True - resulting vector will be L2-normalized (unit euclidean length).\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Vector representation of `word`.\n |      \n |      Raises\n |      ------\n |      KeyError\n |          If word not in vocabulary.\n |  \n |  words_closer_than(self, w1, w2)\n |      Get all words that are closer to `w1` than `w2` is to `w1`.\n |      \n |      Parameters\n |      ----------\n |      w1 : str\n |          Input word.\n |      w2 : str\n |          Input word.\n |      \n |      Returns\n |      -------\n |      list (str)\n |          List of words that are closer to `w1` than `w2` is to `w1`.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods inherited from WordEmbeddingsKeyedVectors:\n |  \n |  cosine_similarities(vector_1, vectors_all)\n |      Compute cosine similarities between one vector and a set of other vectors.\n |      \n |      Parameters\n |      ----------\n |      vector_1 : numpy.ndarray\n |          Vector from which similarities are to be computed, expected shape (dim,).\n |      vectors_all : numpy.ndarray\n |          For each row in vectors_all, distance from vector_1 is computed, expected shape (num_vectors, dim).\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Contains cosine distance between `vector_1` and each row in `vectors_all`, shape (num_vectors,).\n |  \n |  log_accuracy(section)\n |  \n |  log_evaluate_word_pairs(pearson, spearman, oov, pairs)\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from WordEmbeddingsKeyedVectors:\n |  \n |  index2entity\n |  \n |  syn0\n |  \n |  syn0norm\n |  \n |  wv\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from BaseKeyedVectors:\n |  \n |  __getitem__(self, entities)\n |      Get vector representation of `entities`.\n |      \n |      Parameters\n |      ----------\n |      entities : {str, list of str}\n |          Input entity/entities.\n |      \n |      Returns\n |      -------\n |      numpy.ndarray\n |          Vector representation for `entities` (1D if `entities` is string, otherwise - 2D).\n |  \n |  __setitem__(self, entities, weights)\n |      Add entities and theirs vectors in a manual way.\n |      If some entity is already in the vocabulary, old vector is replaced with the new one.\n |      This method is alias for :meth:`~gensim.models.keyedvectors.BaseKeyedVectors.add` with `replace=True`.\n |      \n |      Parameters\n |      ----------\n |      entities : {str, list of str}\n |          Entities specified by their string ids.\n |      weights: list of numpy.ndarray or numpy.ndarray\n |          List of 1D np.array vectors or 2D np.array of vectors.\n |  \n |  add(self, entities, weights, replace=False)\n |      Append entities and theirs vectors in a manual way.\n |      If some entity is already in the vocabulary, the old vector is kept unless `replace` flag is True.\n |      \n |      Parameters\n |      ----------\n |      entities : list of str\n |          Entities specified by string ids.\n |      weights: list of numpy.ndarray or numpy.ndarray\n |          List of 1D np.array vectors or a 2D np.array of vectors.\n |      replace: bool, optional\n |          Flag indicating whether to replace vectors for entities which already exist in the vocabulary,\n |          if True - replace vectors, otherwise - keep old vectors.\n |  \n |  closer_than(self, entity1, entity2)\n |      Get all entities that are closer to `entity1` than `entity2` is to `entity1`.\n |  \n |  most_similar_to_given(self, entity1, entities_list)\n |      Get the `entity` from `entities_list` most similar to `entity1`.\n |  \n |  rank(self, entity1, entity2)\n |      Rank of the distance of `entity2` from `entity1`, in relation to distances of all entities from `entity1`.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from gensim.utils.SaveLoad:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"01804c88-ad00-4f49-a126-e2d547c83d4c"},"source":"dm_word_vecs = dm_model.wv\r\nnp.shape(dm_word_vecs.vectors)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"(9849, 50)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"9d46f2ef-6eed-4eda-b3a0-12f453a9cd11"},"source":"dm_mean_word_vecs = dm_mean_model.wv\r\nnp.shape(dm_mean_word_vecs.vectors)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"(9849, 50)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"ae74026b-4502-4bb0-8f07-8e847482e093"},"source":"dm_concat_word_vecs = dm_concat_model.wv\r\nnp.shape(dm_concat_word_vecs.vectors)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"(9849, 50)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"830c065d-90bb-4804-9909-fd056e35c5cf"},"source":"dbow_word_vecs = dbow_model.wv\r\nnp.shape(dbow_word_vecs.vectors)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"(9849, 50)"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"144c77a1-dac8-4979-a35f-0f7575d27d6e"},"source":"start_time = time.time()\r\n\r\nkmeans = KMeans(n_clusters = 3, random_state = 42, max_iter = 1000)\r\nkmeans.fit(dm_word_vecs.vectors)\r\n\r\nprint('---- Total time to execute: %s seconds ----' %(time.time() - start_time))","execution_count":null,"outputs":[{"name":"stdout","text":"---- Total time to execute: 10.546538829803467 seconds ----\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5d946012-b22e-4efe-a9ec-e210db0295b5"},"source":"def kmeans_clusters(model, data, num_clusters = 3, topn = 10):\r\n    kmeans = KMeans(n_clusters = num_clusters, random_state = 42, max_iter = 500)\r\n    kmeans.fit(data)\r\n    clusters = pd.DataFrame()\r\n    for n in range(num_clusters):\r\n        clusters['cluster_' + str(n)] = [word[0] for word in model.similar_by_vector(kmeans.cluster_centers_[n], topn = topn)]\r\n    \r\n    return clusters\r\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"b153f5a9-857e-495b-ac02-6bcb36d4ffcb"},"source":"kmeans_clusters(model = dm_model, data = dm_word_vecs.vectors, num_clusters = 3, topn = 10)","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n  \n","output_type":"stream"},{"output_type":"execute_result","execution_count":37,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":3,"columns":[{"name":"cluster_0","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coronavirus","count":1},{"name":"weaken","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_1","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"accor","count":1},{"name":"kazuhisa","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_2","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coowner","count":1},{"name":"postmeete","count":1},{"name":"8 others","count":8}]}}],"rows_top":{"0":{"cluster_0":"coronavirus","cluster_1":"accor","cluster_2":"coowner"},"1":{"cluster_0":"weaken","cluster_1":"kazuhisa","cluster_2":"postmeete"},"2":{"cluster_0":"sharply","cluster_1":"golftv","cluster_2":"congresswoman"},"3":{"cluster_0":"outbreak","cluster_1":"savage","cluster_2":"newsweek"},"4":{"cluster_0":"china","cluster_1":"pickle","cluster_2":"najib"},"5":{"cluster_0":"decline","cluster_1":"conveyorbelt","cluster_2":"dea"},"6":{"cluster_0":"rebound","cluster_1":"artois","cluster_2":"stiffen"},"7":{"cluster_0":"slow","cluster_1":"derive","cluster_2":"audibly"},"8":{"cluster_0":"recession","cluster_1":"sixpack","cluster_2":"mcdaniel"},"9":{"cluster_0":"sector","cluster_1":"pennsylvaniabased","cluster_2":"brexiter"}},"rows_bottom":null},"text/plain":"     cluster_0          cluster_1      cluster_2\n0  coronavirus              accor        coowner\n1       weaken           kazuhisa      postmeete\n2      sharply             golftv  congresswoman\n3     outbreak             savage       newsweek\n4        china             pickle          najib\n5      decline       conveyorbelt            dea\n6      rebound             artois        stiffen\n7         slow             derive        audibly\n8    recession            sixpack       mcdaniel\n9       sector  pennsylvaniabased       brexiter","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster_0</th>\n      <th>cluster_1</th>\n      <th>cluster_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coronavirus</td>\n      <td>accor</td>\n      <td>coowner</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>weaken</td>\n      <td>kazuhisa</td>\n      <td>postmeete</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sharply</td>\n      <td>golftv</td>\n      <td>congresswoman</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>outbreak</td>\n      <td>savage</td>\n      <td>newsweek</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>china</td>\n      <td>pickle</td>\n      <td>najib</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>decline</td>\n      <td>conveyorbelt</td>\n      <td>dea</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>rebound</td>\n      <td>artois</td>\n      <td>stiffen</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>slow</td>\n      <td>derive</td>\n      <td>audibly</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>recession</td>\n      <td>sixpack</td>\n      <td>mcdaniel</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>sector</td>\n      <td>pennsylvaniabased</td>\n      <td>brexiter</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"137907a1-8cdb-4829-9a0f-254924a478fd"},"source":"kmeans_clusters(model = dm_mean_model, data = dm_mean_word_vecs.vectors)","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n  \n","output_type":"stream"},{"output_type":"execute_result","execution_count":44,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":3,"columns":[{"name":"cluster_0","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coronavirus","count":1},{"name":"sharply","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_1","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"beth","count":1},{"name":"congresswoman","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_2","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"kazuhisa","count":1},{"name":"golftv","count":1},{"name":"8 others","count":8}]}}],"rows_top":{"0":{"cluster_0":"coronavirus","cluster_1":"beth","cluster_2":"kazuhisa"},"1":{"cluster_0":"sharply","cluster_1":"congresswoman","cluster_2":"golftv"},"2":{"cluster_0":"rebound","cluster_1":"gymnastics","cluster_2":"accor"},"3":{"cluster_0":"recession","cluster_1":"coowner","cluster_2":"filter"},"4":{"cluster_0":"outbreak","cluster_1":"kalvaria","cluster_2":"konami"},"5":{"cluster_0":"slow","cluster_1":"najib","cluster_2":"breezes"},"6":{"cluster_0":"weaken","cluster_1":"postmeete","cluster_2":"derive"},"7":{"cluster_0":"global","cluster_1":"newsweek","cluster_2":"mishap"},"8":{"cluster_0":"february","cluster_1":"tester","cluster_2":"pickle"},"9":{"cluster_0":"economy","cluster_1":"rogers","cluster_2":"krystal"}},"rows_bottom":null},"text/plain":"     cluster_0      cluster_1 cluster_2\n0  coronavirus           beth  kazuhisa\n1      sharply  congresswoman    golftv\n2      rebound     gymnastics     accor\n3    recession        coowner    filter\n4     outbreak       kalvaria    konami\n5         slow          najib   breezes\n6       weaken      postmeete    derive\n7       global       newsweek    mishap\n8     february         tester    pickle\n9      economy         rogers   krystal","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster_0</th>\n      <th>cluster_1</th>\n      <th>cluster_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coronavirus</td>\n      <td>beth</td>\n      <td>kazuhisa</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sharply</td>\n      <td>congresswoman</td>\n      <td>golftv</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>rebound</td>\n      <td>gymnastics</td>\n      <td>accor</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>recession</td>\n      <td>coowner</td>\n      <td>filter</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>outbreak</td>\n      <td>kalvaria</td>\n      <td>konami</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>slow</td>\n      <td>najib</td>\n      <td>breezes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>weaken</td>\n      <td>postmeete</td>\n      <td>derive</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>global</td>\n      <td>newsweek</td>\n      <td>mishap</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>february</td>\n      <td>tester</td>\n      <td>pickle</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>economy</td>\n      <td>rogers</td>\n      <td>krystal</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"a61a5100-a14e-453f-a9ad-f988dc1d98a3"},"source":"kmeans_clusters(model = dm_concat_model, data = dm_concat_word_vecs.vectors)","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n  \n","output_type":"stream"},{"output_type":"execute_result","execution_count":45,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":3,"columns":[{"name":"cluster_0","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coronavirus","count":1},{"name":"rebound","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_1","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coowner","count":1},{"name":"stiffen","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_2","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"accor","count":1},{"name":"golftv","count":1},{"name":"8 others","count":8}]}}],"rows_top":{"0":{"cluster_0":"coronavirus","cluster_1":"coowner","cluster_2":"accor"},"1":{"cluster_0":"rebound","cluster_1":"stiffen","cluster_2":"golftv"},"2":{"cluster_0":"sharply","cluster_1":"beg","cluster_2":"ibis"},"3":{"cluster_0":"weaken","cluster_1":"postmeete","cluster_2":"savage"},"4":{"cluster_0":"slow","cluster_1":"catholic","cluster_2":"kazuhisa"},"5":{"cluster_0":"disruption","cluster_1":"congresswoman","cluster_2":"breezes"},"6":{"cluster_0":"china","cluster_1":"bizarre","cluster_2":"hgtv"},"7":{"cluster_0":"january","cluster_1":"najib","cluster_2":"thc"},"8":{"cluster_0":"outbreak","cluster_1":"polite","cluster_2":"mishap"},"9":{"cluster_0":"recession","cluster_1":"audibly","cluster_2":"leonardo"}},"rows_bottom":null},"text/plain":"     cluster_0      cluster_1 cluster_2\n0  coronavirus        coowner     accor\n1      rebound        stiffen    golftv\n2      sharply            beg      ibis\n3       weaken      postmeete    savage\n4         slow       catholic  kazuhisa\n5   disruption  congresswoman   breezes\n6        china        bizarre      hgtv\n7      january          najib       thc\n8     outbreak         polite    mishap\n9    recession        audibly  leonardo","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster_0</th>\n      <th>cluster_1</th>\n      <th>cluster_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coronavirus</td>\n      <td>coowner</td>\n      <td>accor</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>rebound</td>\n      <td>stiffen</td>\n      <td>golftv</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sharply</td>\n      <td>beg</td>\n      <td>ibis</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>weaken</td>\n      <td>postmeete</td>\n      <td>savage</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>slow</td>\n      <td>catholic</td>\n      <td>kazuhisa</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>disruption</td>\n      <td>congresswoman</td>\n      <td>breezes</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>china</td>\n      <td>bizarre</td>\n      <td>hgtv</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>january</td>\n      <td>najib</td>\n      <td>thc</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>outbreak</td>\n      <td>polite</td>\n      <td>mishap</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>recession</td>\n      <td>audibly</td>\n      <td>leonardo</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"476d6053-e98b-44be-bcb0-b69452f76f30"},"source":"kmeans_clusters(model = dbow_model, data = dbow_word_vecs.vectors)","execution_count":null,"outputs":[{"name":"stderr","text":"/opt/venv/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `similar_by_vector` (Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead).\n  \n","output_type":"stream"},{"output_type":"execute_result","execution_count":46,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":10,"column_count":3,"columns":[{"name":"cluster_0","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"coronavirus","count":1},{"name":"sharply","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_1","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"najib","count":1},{"name":"mcdaniel","count":1},{"name":"8 others","count":8}]}},{"name":"cluster_2","dtype":"object","stats":{"unique_count":10,"nan_count":0,"categories":[{"name":"accor","count":1},{"name":"kazuhisa","count":1},{"name":"8 others","count":8}]}}],"rows_top":{"0":{"cluster_0":"coronavirus","cluster_1":"najib","cluster_2":"accor"},"1":{"cluster_0":"sharply","cluster_1":"mcdaniel","cluster_2":"kazuhisa"},"2":{"cluster_0":"slow","cluster_1":"rogers","cluster_2":"golftv"},"3":{"cluster_0":"outbreak","cluster_1":"kalvaria","cluster_2":"simmon"},"4":{"cluster_0":"january","cluster_1":"postmeete","cluster_2":"breezes"},"5":{"cluster_0":"recession","cluster_1":"beth","cluster_2":"derive"},"6":{"cluster_0":"year","cluster_1":"coowner","cluster_2":"stalker"},"7":{"cluster_0":"economy","cluster_1":"gasp","cluster_2":"teriyaki"},"8":{"cluster_0":"decline","cluster_1":"audibly","cluster_2":"pennsylvaniabased"},"9":{"cluster_0":"ease","cluster_1":"dea","cluster_2":"savage"}},"rows_bottom":null},"text/plain":"     cluster_0  cluster_1          cluster_2\n0  coronavirus      najib              accor\n1      sharply   mcdaniel           kazuhisa\n2         slow     rogers             golftv\n3     outbreak   kalvaria             simmon\n4      january  postmeete            breezes\n5    recession       beth             derive\n6         year    coowner            stalker\n7      economy       gasp           teriyaki\n8      decline    audibly  pennsylvaniabased\n9         ease        dea             savage","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cluster_0</th>\n      <th>cluster_1</th>\n      <th>cluster_2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>coronavirus</td>\n      <td>najib</td>\n      <td>accor</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sharply</td>\n      <td>mcdaniel</td>\n      <td>kazuhisa</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>slow</td>\n      <td>rogers</td>\n      <td>golftv</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>outbreak</td>\n      <td>kalvaria</td>\n      <td>simmon</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>january</td>\n      <td>postmeete</td>\n      <td>breezes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>recession</td>\n      <td>beth</td>\n      <td>derive</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>year</td>\n      <td>coowner</td>\n      <td>stalker</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>economy</td>\n      <td>gasp</td>\n      <td>teriyaki</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>decline</td>\n      <td>audibly</td>\n      <td>pennsylvaniabased</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>ease</td>\n      <td>dea</td>\n      <td>savage</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"5c8a9739-456d-4597-8d25-fd414bea2125"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"aff6a65b-2de9-440d-8132-38996f0b7b3d"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"e8cd6891-d057-4c5e-a47b-3b7ca300e900"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"0d3944ac-4a51-485d-8046-a00578110805"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"429d6ec2-f0d1-4e9e-9f43-dda3650af23e"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":2,"metadata":{"orig_nbformat":2,"deepnote_notebook_id":"c094927c-9b01-4951-8d2b-b32975ac284a","deepnote_execution_queue":[]}}